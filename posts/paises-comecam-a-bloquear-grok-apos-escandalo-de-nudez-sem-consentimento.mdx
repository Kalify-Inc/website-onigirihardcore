---
date: "2026-01-12:14:50:00"
title: Países começam a bloquear Grok após escândalo de nudez sem consentimento
description: "A Indonésia suspendeu o Grok no último sábado (10/01), movimento seguido pela Malásia no dia seguinte, tornando-os os dois primeiros países a tomar medidas do tipo. O chatbot de IA da xAI, de Elon Musk, foi largamente usado na primeira semana de 2026 para gerar e publicar imagens explícitas de mulheres e crianças."
image: https://pbs.twimg.com/media/GfRtoB4asAAL1Mk.jpg
categories: "Technologies"
ytid: ""
author: 'Anderson "Yagasaki" Marlon'
---

A Indonésia suspendeu o Grok no último sábado (10/01), movimento seguido pela Malásia no dia seguinte, tornando-os os dois primeiros países a tomar medidas do tipo. O chatbot de IA da xAI, de Elon Musk, foi largamente usado na primeira semana de 2026 para gerar e publicar imagens explícitas de mulheres e crianças.

A decisão da Indonésia foi tomada no sábado (10) pelo Ministério das Comunicações e do Digital (Komdigi). O comunicado diz que “o acesso ao aplicativo Grok foi temporariamente cortado”. O chatbot está disponível na rede social X, na web e como aplicativo — a pasta não especifica se a medida vale para todas as modalidades.

O Komdigi diz que a “prática de deepfakes não consensuais é uma séria violação dos direitos humanos, dignidade e segurança dos cidadãos no espaço digital”. As autoridades também solicitaram que o X se apresente em breve para prestar esclarecimentos sobre os impactos negativos do uso do Grok.

No domingo (11), foi a vez de a Malásia impor barreiras ao robô da xAI. A Comissão de Comunicações e Multimídia (MCMC) publicou uma restrição temporária no acesso ao Grok por usuários do país. Assim como na Indonésia, não está clara a abrangência da medida — se ela vale para o Grok no X, na web ou nos apps.

O órgão diz ter notificado o X e a xAI em 3 e 8 de janeiro para solicitar a implementação de barreiras para impedir a geração de conteúdos em desacordo com as leis malaias. As empresas responderam apresentando mecanismos baseados em denúncias dos próprios usuários, o que foi considerado insuficiente para prevenir os danos.

“Essa ação vem depois de usos inadequados repetidos do Grok para gerar sem consentimento imagens manipuladas obscenas, sexualmente explícitas, indecentes e ofensivas, incluindo conteúdos envolvendo mulheres e menores de idade”, explica o MCMC.

## Por que o Grok foi bloqueado?

O chatbot de inteligência artificial generativa da xAI, de Elon Musk, vem sendo usado para gerar imagens explícitas sem consentimento das pessoas envolvidas — em alguns casos, menores de idade.

No X, é possível responder a uma foto marcando o chatbot e pedir que ele gere uma nova imagem de acordo com o prompt. Muitos usuários têm usado a funcionalidade para “despir” mulheres, adolescentes e crianças.

![](https://files.tecnoblog.net/wp-content/uploads/2026/01/image-1060x596.png)

Segundo um levantamento da Bloomberg, o Grok gerou 6.700 imagens sexualmente sugestivas por hora nos dias 5 e 6 de janeiro. O conteúdo ficava disponível publicamente na conta do chatbot.

Após críticas, o X restringiu a funcionalidade de geração de imagens a usuários verificados, que pagam assinatura. A mudança teve reações negativas de autoridades, que consideraram que a medida não resolve o problema, apenas o torna um “serviço premium”.

## Outros países podem bloquear o Grok?

Atualmente, a geração de imagens de nudez sem consentimento vem sendo analisada pela União Europeia, pelo Reino Unido, pela Índia e pela França.

No Brasil, a Polícia Civil do Rio de Janeiro cobrou esclarecimentos do X sobre o uso do Grok para gerar esse tipo de conteúdo. A investigação parte de um caso específico de uma usuária que publicou uma foto e teve sua imagem transformada em conteúdo sexual dezenas de vezes.

Procurada pela Al Jazeera, a xAI enviou uma resposta automática: “A mídia tradicional mente” (”Legacy media lies”). Posteriormente, um porta-voz da empresa enviou um posicionamento que diz que pessoas que usam o Grok para criar conteúdo ilegal sofrerão consequências judiciais.
